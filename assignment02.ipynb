{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9UJNNuc62b9"
      },
      "source": [
        "# **Secure AI Systems : Red and Blue Teaming an MNIST Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1TjqsCV9WbU"
      },
      "source": [
        "**Task - 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i7mjnaaVTBF5"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from PIL import Image, ImageDraw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ibZqJW4WTIqi"
      },
      "outputs": [],
      "source": [
        "# ========== Configuration ==========\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 6\n",
        "LR = 1e-3\n",
        "SEED = 42\n",
        "POISON_COUNT = 100  # number of poisoned training samples\n",
        "POISON_LABEL = 7\n",
        "POISON_BOX_SIZE = 3\n",
        "POISON_BOX_COLOR = (255, 0, 0)\n",
        "RANDOM_STATE = np.random.RandomState(SEED)\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H-0hY1ozTJsU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== Data transformation ==========\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # from [0,255] to [0,1], shape [1,28,28]\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L-2ge5h9q04",
        "outputId": "c2d6919c-9969-4aa9-ea67-a8fa1cc67421"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 62.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.71MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.6MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.11MB/s]\n"
          ]
        }
      ],
      "source": [
        "# ========== Download MNIST Dataset ==========\n",
        "data_root = './data'\n",
        "train_set = MNIST(root=data_root, train=True, download=True, transform=transform)\n",
        "test_set = MNIST(root=data_root, train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jiwUfxz2TQyD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== Utils: Small CNN ==========\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 14x14\n",
        "            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # 7x7\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*7*7, 128), nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6illSvcnTToB"
      },
      "outputs": [],
      "source": [
        "# ========== Utils: training/eval ==========\n",
        "def train_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_y = []\n",
        "    all_pred = []\n",
        "    for x,y in loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_y.append(y.cpu().numpy())\n",
        "        all_pred.append(pred.cpu().numpy())\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "    all_y = np.concatenate(all_y)\n",
        "    all_pred = np.concatenate(all_pred)\n",
        "    acc = correct/total\n",
        "    return running_loss / total, acc, all_y, all_pred\n",
        "\n",
        "def measure_inference_time(model, loader, n_batches=20):\n",
        "    model.eval()\n",
        "    t0 = time.time()\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y) in enumerate(loader):\n",
        "            if i>=n_batches: break\n",
        "            x = x.to(DEVICE)\n",
        "            _ = model(x)\n",
        "            count += x.size(0)\n",
        "    t1 = time.time()\n",
        "    total_time = t1 - t0\n",
        "    return total_time, total_time / max(1,count)  # total, per-sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ALz-O1Tce4",
        "outputId": "48c9b9f0-f1df-432e-b7ad-00525ad98012"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6: train_loss=0.2906, train_acc=0.9104, val_loss=0.0592, val_acc=0.9801\n",
            "Epoch 2/6: train_loss=0.0736, train_acc=0.9782, val_loss=0.0445, val_acc=0.9860\n",
            "Epoch 3/6: train_loss=0.0544, train_acc=0.9835, val_loss=0.0346, val_acc=0.9886\n",
            "Epoch 4/6: train_loss=0.0417, train_acc=0.9873, val_loss=0.0332, val_acc=0.9889\n",
            "Epoch 5/6: train_loss=0.0357, train_acc=0.9884, val_loss=0.0276, val_acc=0.9905\n",
            "Epoch 6/6: train_loss=0.0293, train_acc=0.9907, val_loss=0.0251, val_acc=0.9919\n"
          ]
        }
      ],
      "source": [
        "# ========== Training baseline ==========\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "model = SimpleCNN().to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[]}\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_acc, _, _ = evaluate(model, test_loader, criterion)\n",
        "    history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2zCGPnr7Tjgn"
      },
      "outputs": [],
      "source": [
        "# Save baseline model\n",
        "os.makedirs('models', exist_ok=True)\n",
        "torch.save(model.state_dict(), 'models/cnn_mnist_baseline.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeNSCyUp_flq"
      },
      "source": [
        "**Task - 2 [Evaluate model's performance on a clean test set]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIUcrYGYUsXo",
        "outputId": "e3f5d5ca-b2f2-4077-818f-b27cd553bbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline test acc: 0.9919 loss: 0.025104037945030723\n",
            "Inference total (first 50 batches): 5.419584512710571 per-sample: 0.0005419584512710571\n"
          ]
        }
      ],
      "source": [
        "# Evaluate baseline\n",
        "val_loss, val_acc, all_y, all_pred = evaluate(model, test_loader, criterion)\n",
        "tot_time, per_sample_time = measure_inference_time(model, test_loader, n_batches=50)\n",
        "print(\"Baseline test acc:\", val_acc, \"loss:\", val_loss)\n",
        "print(\"Inference total (first 50 batches):\", tot_time, \"per-sample:\", per_sample_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mQF6pVlUu52",
        "outputId": "9f1ac712-9943-4769-bf3f-c3f5c99c15a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9939    0.9949    0.9944       980\n",
            "           1     0.9965    0.9974    0.9969      1135\n",
            "           2     0.9903    0.9932    0.9918      1032\n",
            "           3     0.9872    0.9950    0.9911      1010\n",
            "           4     0.9929    0.9919    0.9924       982\n",
            "           5     0.9933    0.9922    0.9927       892\n",
            "           6     0.9969    0.9916    0.9942       958\n",
            "           7     0.9856    0.9961    0.9908      1028\n",
            "           8     0.9897    0.9856    0.9877       974\n",
            "           9     0.9930    0.9802    0.9865      1009\n",
            "\n",
            "    accuracy                         0.9919     10000\n",
            "   macro avg     0.9919    0.9918    0.9919     10000\n",
            "weighted avg     0.9919    0.9919    0.9919     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Confusion Matrix and report\n",
        "cm = confusion_matrix(all_y, all_pred)\n",
        "print(\"Classification Report:\\n\", classification_report(all_y, all_pred, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BexxKFV6qWPH"
      },
      "source": [
        "**Red Team Experiments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F473XkAi_oPl"
      },
      "source": [
        "**Task -5 [Data Poisoning]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DceY5PDsUyPF"
      },
      "outputs": [],
      "source": [
        "# ========== POISONING: add small colored square to ~100 images of label 7 in training set ==========\n",
        "# Function to add colored square to a PIL image; input is tensor [1,28,28]\n",
        "def add_corner_square(img_tensor, box_size=3, color=(255,0,0)):\n",
        "    # img_tensor: torch tensor (1,28,28) in [0,1]\n",
        "    arr = (img_tensor.squeeze(0).cpu().numpy() * 255).astype(np.uint8)\n",
        "    pil = Image.fromarray(arr, mode='L').convert('RGB')  # convert to RGB\n",
        "    draw = ImageDraw.Draw(pil)\n",
        "    # place square in bottom-right corner (you can pick top-left)\n",
        "    x0, y0 = pil.size[0] - box_size - 1, pil.size[1] - box_size - 1\n",
        "    x1, y1 = x0 + box_size, y0 + box_size\n",
        "    draw.rectangle([x0,y0,x1,y1], fill=color)\n",
        "    # convert back to grayscale tensor\n",
        "    pil_gray = pil.convert('L')\n",
        "    arr2 = np.array(pil_gray).astype(np.float32) / 255.0\n",
        "    return torch.from_numpy(arr2).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WKvNXYzU1Y3",
        "outputId": "5d10695d-c1e4-4f21-e01c-0f3400c195e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created poisoned set of size 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1381052354.py:6: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  pil = Image.fromarray(arr, mode='L').convert('RGB')  # convert to RGB\n"
          ]
        }
      ],
      "source": [
        "# Create poisoned subset\n",
        "def create_poisoned_dataset(original_dataset, poison_count=100):\n",
        "    # Find indices of label==7\n",
        "    indices = [i for i in range(len(original_dataset)) if original_dataset[i][1]==POISON_LABEL]\n",
        "    chosen = RANDOM_STATE.choice(indices, size=poison_count, replace=False)\n",
        "    poisoned_examples = []\n",
        "    poisoned_targets = []\n",
        "    # Original dataset returns (tensor, label)\n",
        "    for idx in chosen:\n",
        "        img, label = original_dataset[idx]\n",
        "        poisoned_img = add_corner_square(img, box_size=POISON_BOX_SIZE, color=POISON_BOX_COLOR)\n",
        "        poisoned_examples.append((poisoned_img, label))\n",
        "    # Build a Dataset wrapper for poisoned examples\n",
        "    class PoisonedDataset(Dataset):\n",
        "        def __init__(self, examples):\n",
        "            self.examples = examples\n",
        "        def __len__(self): return len(self.examples)\n",
        "        def __getitem__(self, i): return self.examples[i]\n",
        "    return PoisonedDataset(poisoned_examples), chosen\n",
        "\n",
        "poisoned_ds, poisoned_idx_list = create_poisoned_dataset(train_set, POISON_COUNT)\n",
        "print(\"Created poisoned set of size\", len(poisoned_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xSDd8l5aU7cK"
      },
      "outputs": [],
      "source": [
        "# Replace those indices in the original training set with poisoned images (method 1)\n",
        "class TrainSetWithPoison(Dataset):\n",
        "    def __init__(self, base_dataset, poison_examples, poison_indices):\n",
        "        self.base = base_dataset\n",
        "        self.poison_map = dict(zip(poison_indices, poison_examples))\n",
        "    def __len__(self): return len(self.base)\n",
        "    def __getitem__(self, idx):\n",
        "        if idx in self.poison_map:\n",
        "            return self.poison_map[idx]\n",
        "        else:\n",
        "            return self.base[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "S4FjC3k4U_Dm"
      },
      "outputs": [],
      "source": [
        "# Build list of (tensor,label) for poisoned examples matching the indices\n",
        "poison_examples_for_map = [poisoned_ds[i] for i in range(len(poisoned_ds))]\n",
        "train_poisoned_dataset = TrainSetWithPoison(train_set, poison_examples_for_map, poisoned_idx_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PF0uAT5rVBkX"
      },
      "outputs": [],
      "source": [
        "# New DataLoader with poisoned training data\n",
        "train_poisoned_loader = DataLoader(train_poisoned_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D424l_bCB6Dc"
      },
      "source": [
        "**Task -6 [Test the CNN model again with the new test data and report the model performance]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0nd0I0wVENC",
        "outputId": "dcd70767-12a6-4b29-e9ae-aaeaf053af05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1381052354.py:6: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  pil = Image.fromarray(arr, mode='L').convert('RGB')  # convert to RGB\n"
          ]
        }
      ],
      "source": [
        "# ========== RE-TRAIN (red-team evaluation)  baseline model on poisoned test set variant ==========\n",
        "# Option A: Evaluate baseline on test images with squares added (test-time attack)\n",
        "def create_poisoned_testset(test_dataset, add_square_to_all_of_digit=7):\n",
        "    poisoned_examples = []\n",
        "    poisoned_targets = []\n",
        "    for i in range(len(test_dataset)):\n",
        "        img, label = test_dataset[i]\n",
        "        if label == add_square_to_all_of_digit:\n",
        "            img2 = add_corner_square(img, box_size=POISON_BOX_SIZE, color=POISON_BOX_COLOR)\n",
        "            poisoned_examples.append((img2, label))\n",
        "        else:\n",
        "            poisoned_examples.append((img, label))\n",
        "    class SimpleDS(Dataset):\n",
        "        def __init__(self, ex): self.ex = ex\n",
        "        def __len__(self): return len(self.ex)\n",
        "        def __getitem__(self, i): return self.ex[i]\n",
        "    return SimpleDS(poisoned_examples)\n",
        "\n",
        "test_poisoned = create_poisoned_testset(test_set, add_square_to_all_of_digit=POISON_LABEL)\n",
        "test_poisoned_loader = DataLoader(test_poisoned, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rBiNcMSVJOs",
        "outputId": "4f3e2fa4-4630-4f21-893f-d9c19eb4b170"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline model on poisoned test set: acc= 0.9917 loss= 0.025098511978110763\n"
          ]
        }
      ],
      "source": [
        "# Evaluate baseline on poisoned test set\n",
        "poison_val_loss, poison_val_acc, py, ppr = evaluate(model, test_poisoned_loader, criterion)\n",
        "print(\"Baseline model on poisoned test set: acc=\", poison_val_acc, \"loss=\", poison_val_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cniB5C9Ve7z",
        "outputId": "f555b224-d202-4b8b-82d2-8d9f573c6bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.20.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (75.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.6.0)\n",
            "Downloading adversarial_robustness_toolbox-1.20.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: adversarial-robustness-toolbox\n",
            "Successfully installed adversarial-robustness-toolbox-1.20.1\n"
          ]
        }
      ],
      "source": [
        "#!pip uninstall -y adversarial-robustness-toolbox scikit-learn\n",
        "#!pip install -U pip setuptools wheel\n",
        "!pip install -U adversarial-robustness-toolbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IIwITg8FVMQN"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# ========== ADVERSARIAL ATTACK: FGSM using ART (PyTorch) ==========\n",
        "# ART requires a wrapper around model\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "\n",
        "def art_wrap_and_generate(model, loader, eps=0.2, max_examples=2000):\n",
        "    # Wrap model into ART classifier\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer_dummy = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    classifier = PyTorchClassifier(\n",
        "        model=model,\n",
        "        loss=criterion,\n",
        "        optimizer=optimizer_dummy,\n",
        "        input_shape=(1,28,28),\n",
        "        nb_classes=10,\n",
        "        device_type='gpu' if torch.cuda.is_available() else 'cpu'\n",
        "    )\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    count = 0\n",
        "    for x,y in loader:\n",
        "        x_np = x.numpy()\n",
        "        y_np = y.numpy()\n",
        "        x_list.append(x_np)\n",
        "        y_list.append(y_np)\n",
        "        count += x_np.shape[0]\n",
        "        if count >= max_examples:\n",
        "            break\n",
        "    x_all = np.concatenate(x_list, axis=0)\n",
        "    y_all = np.concatenate(y_list, axis=0)\n",
        "    fgsm = FastGradientMethod(estimator=classifier, eps=eps)\n",
        "    x_adv = fgsm.generate(x_all)\n",
        "    return x_all, y_all, x_adv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTapHFhkXYjK",
        "outputId": "00330cf9-88fb-4ab6-a344-df60d796ba52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated adv examples shape: (2048, 1, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "###\n",
        "# Generate adversarial examples from a subset of test set\n",
        "x_clean, y_clean, x_adv = art_wrap_and_generate(model, test_loader, eps=0.2, max_examples=2000)\n",
        "print(\"Generated adv examples shape:\", x_adv.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x-WmjO33ydo",
        "outputId": "40a3dbc8-cd00-46e7-c5f0-86c3e3c3799c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline acc on clean subset: 0.99072265625 on adv: 0.244140625\n"
          ]
        }
      ],
      "source": [
        "###\n",
        "# Evaluate on adversarial set using the model\n",
        "def eval_on_numpy_batch(model, x_np, y_np, batch_size=256):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, x_np.shape[0], batch_size):\n",
        "            xbatch = torch.from_numpy(x_np[i:i+batch_size]).to(DEVICE).float()\n",
        "            out = model(xbatch)\n",
        "            preds.append(out.argmax(dim=1).cpu().numpy())\n",
        "    preds = np.concatenate(preds)\n",
        "    acc = (preds == y_np).mean()\n",
        "    return acc, preds\n",
        "\n",
        "adv_acc, adv_preds = eval_on_numpy_batch(model, x_adv, y_clean)\n",
        "clean_acc, clean_preds = eval_on_numpy_batch(model, x_clean, y_clean)\n",
        "print(\"Baseline acc on clean subset:\", clean_acc, \"on adv:\", adv_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O21yEcdBpgCZ"
      },
      "source": [
        "**Task -7 [Protection (Blue Teaming)]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lBwyja3d32RJ"
      },
      "outputs": [],
      "source": [
        "# ========= BLUE TEAM: Adversarial training using adv examples and clean subset data ==========\n",
        "# Build a new dataset from original train set and adversarial examples\n",
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, x_np, y_np):\n",
        "        self.x = x_np\n",
        "        self.y = y_np\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, i):\n",
        "        return torch.from_numpy(self.x[i]).float(), int(self.y[i])\n",
        "\n",
        "adv_train_ds = NumpyDataset(x_adv, y_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Lth5dVfz37nA"
      },
      "outputs": [],
      "source": [
        "# Combine with original train_set (or subset)\n",
        "combined = ConcatDataset([train_set, adv_train_ds])\n",
        "combined_loader = DataLoader(combined, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SMhw1ez3_1D",
        "outputId": "d72ebbf2-4d6e-4754-f777-d1282b1095be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[AdvTrain] Epoch 1/3: train_acc=0.9096 val_acc=0.9834\n",
            "[AdvTrain] Epoch 2/3: train_acc=0.9747 val_acc=0.9865\n",
            "[AdvTrain] Epoch 3/3: train_acc=0.9823 val_acc=0.9891\n"
          ]
        }
      ],
      "source": [
        "# Initialize a fresh model for adversarial training\n",
        "model_adv = SimpleCNN().to(DEVICE)\n",
        "optimizer2 = optim.Adam(model_adv.parameters(), lr=LR)\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "EPOCHS_ADV = 3\n",
        "for epoch in range(EPOCHS_ADV):\n",
        "    t_loss, t_acc = train_epoch(model_adv, combined_loader, criterion2, optimizer2)\n",
        "    v_loss, v_acc, _, _ = evaluate(model_adv, test_loader, criterion2)\n",
        "    print(f\"[AdvTrain] Epoch {epoch+1}/{EPOCHS_ADV}: train_acc={t_acc:.4f} val_acc={v_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBrrsae_4Awm",
        "outputId": "14a2f2de-5557-45e6-91ef-be946547beda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After adversarial training - acc on adv examples: 0.984375 acc on clean subset: 0.98681640625\n"
          ]
        }
      ],
      "source": [
        "# Evaluate adv-trained model on adversarial examples\n",
        "adv_trained_acc, _ = eval_on_numpy_batch(model_adv, x_adv, y_clean)\n",
        "clean_trained_acc, _ = eval_on_numpy_batch(model_adv, x_clean, y_clean)\n",
        "print(\"After adversarial training - acc on adv examples:\", adv_trained_acc, \"acc on clean subset:\", clean_trained_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Pri67YQ85MhR"
      },
      "outputs": [],
      "source": [
        "# Save adv-trained model\n",
        "torch.save(model_adv.state_dict(), 'models/cnn_mnist_advtrained.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnm_O-2ElmE4"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
